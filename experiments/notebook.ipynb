{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7d1ef42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Only 1 device found. Replication across devices is trivial but less illustrative.\n",
      "Created Mesh: Mesh('data': 1, axis_types=(Auto,))\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.sharding import Mesh, PartitionSpec, NamedSharding\n",
    "from jax.experimental import mesh_utils\n",
    "\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "import numpy as np\n",
    "import functools\n",
    "\n",
    "# --- Configuration ---\n",
    "INPUT_FEATURES = 64\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = 10\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_STEPS = 100\n",
    "\n",
    "# --- Device Setup ---\n",
    "NUM_DEVICES = jax.local_device_count()\n",
    "if NUM_DEVICES < 2:\n",
    "    print(f\"WARNING: Only {NUM_DEVICES} device found. \"\n",
    "          \"Replication across devices is trivial but less illustrative.\")\n",
    "\n",
    "# Global batch size should be divisible by number of devices for data parallelism\n",
    "GLOBAL_BATCH_SIZE = 32 * NUM_DEVICES\n",
    "DEVICE_BATCH_SIZE = GLOBAL_BATCH_SIZE // NUM_DEVICES\n",
    "\n",
    "# --- 1. Setup Device Mesh ---\n",
    "# Create a 1D mesh for data parallelism. The axis 'data' will be used\n",
    "# to shard the batch dimension and to average gradients.\n",
    "device_mesh = mesh_utils.create_device_mesh((NUM_DEVICES,))\n",
    "mesh = Mesh(devices=device_mesh, axis_names=('data',))\n",
    "print(f\"Created Mesh: {mesh}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dea8a297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Replicated Sharding: NamedSharding(mesh=Mesh('data': 1, axis_types=(Auto,)), spec=PartitionSpec(None,), memory_kind=unpinned_host)\n",
      "Data Sharding: NamedSharding(mesh=Mesh('data': 1, axis_types=(Auto,)), spec=PartitionSpec('data', None), memory_kind=unpinned_host)\n",
      "Label Sharding: NamedSharding(mesh=Mesh('data': 1, axis_types=(Auto,)), spec=PartitionSpec('data',), memory_kind=unpinned_host)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 2. Define Partitioning Specifications ---\n",
    "P = PartitionSpec # Alias for convenience\n",
    "\n",
    "# Specification for Replication: Don't shard along any mesh axis.\n",
    "# P(None,) means don't shard along the first (and only) mesh axis 'data'.\n",
    "replicated_spec = P(None,)\n",
    "\n",
    "# Specification for Data Sharding (Batch Dimension):\n",
    "# Shard the first dimension (batch) along the 'data' mesh axis.\n",
    "# Keep other dimensions (features) un-sharded (replicated within a device).\n",
    "data_sharding_spec = P('data', None) # Shard axis 0, replicate axis 1\n",
    "label_sharding_spec = P('data',)     # Shard axis 0 (for 1D labels)\n",
    "\n",
    "# --- Create NamedShardings (binding Specs to the Mesh) ---\n",
    "replicated_sharding = NamedSharding(mesh, replicated_spec)\n",
    "data_sharding = NamedSharding(mesh, data_sharding_spec)\n",
    "label_sharding = NamedSharding(mesh, label_sharding_spec)\n",
    "\n",
    "print(f\"\\nReplicated Sharding: {replicated_sharding}\")\n",
    "print(f\"Data Sharding: {data_sharding}\")\n",
    "print(f\"Label Sharding: {label_sharding}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25d16d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Define the Neural Network (Same as before) ---\n",
    "class SimpleMLP(nn.Module):\n",
    "    hidden_dim: int\n",
    "    output_dim: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(features=self.hidden_dim, name=\"dense_layer_1\")(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=self.output_dim, name=\"dense_layer_2\")(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25acb0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameter Sharding Tree (all replicated):\n",
      "{'dense_layer_1': {'bias': NamedSharding(mesh=Mesh('data': 1, axis_types=(Auto,)), spec=PartitionSpec(None,), memory_kind=unpinned_host), 'kernel': NamedSharding(mesh=Mesh('data': 1, axis_types=(Auto,)), spec=PartitionSpec(None,), memory_kind=unpinned_host)}, 'dense_layer_2': {'bias': NamedSharding(mesh=Mesh('data': 1, axis_types=(Auto,)), spec=PartitionSpec(None,), memory_kind=unpinned_host), 'kernel': NamedSharding(mesh=Mesh('data': 1, axis_types=(Auto,)), spec=PartitionSpec(None,), memory_kind=unpinned_host)}}\n",
      "\n",
      "Initializing parameters with replicated sharding...\n",
      "Parameter initialization complete.\n",
      "Verifying Parameter Sharding:\n",
      "  Param: (DictKey(key='dense_layer_1'), DictKey(key='bias')), Shape: (128,), Sharding: NamedSharding(mesh=Mesh('data': 1, axis_types=(Auto,)), spec=PartitionSpec(None,), memory_kind=unpinned_host)\n",
      "  Param: (DictKey(key='dense_layer_1'), DictKey(key='kernel')), Shape: (64, 128), Sharding: NamedSharding(mesh=Mesh('data': 1, axis_types=(Auto,)), spec=PartitionSpec(None,), memory_kind=unpinned_host)\n",
      "  Param: (DictKey(key='dense_layer_2'), DictKey(key='bias')), Shape: (10,), Sharding: NamedSharding(mesh=Mesh('data': 1, axis_types=(Auto,)), spec=PartitionSpec(None,), memory_kind=unpinned_host)\n",
      "  Param: (DictKey(key='dense_layer_2'), DictKey(key='kernel')), Shape: (128, 10), Sharding: NamedSharding(mesh=Mesh('data': 1, axis_types=(Auto,)), spec=PartitionSpec(None,), memory_kind=unpinned_host)\n",
      "Parameter sharding verified.\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Initialize Model and Optimizer with Replication ---\n",
    "key = jax.random.PRNGKey(0)\n",
    "model_key, params_key, data_key = jax.random.split(key, 3)\n",
    "\n",
    "model = SimpleMLP(hidden_dim=HIDDEN_DIM, output_dim=OUTPUT_DIM)\n",
    "\n",
    "# Create dummy input *per device* shape for initialization if needed,\n",
    "# though global often works if init isn't shape-dependent.\n",
    "# Using device shape avoids potential issues if init logic uses input shape.\n",
    "dummy_device_input = jnp.ones([DEVICE_BATCH_SIZE, INPUT_FEATURES])\n",
    "\n",
    "# Get abstract structure first (runs on host)\n",
    "abstract_variables = jax.eval_shape(model.init, model_key, dummy_device_input)\n",
    "abstract_params = abstract_variables['params']\n",
    "\n",
    "# Create the sharding specification tree for parameters - all replicated\n",
    "# Use tree.map to apply the replicated_sharding to every leaf in the params tree\n",
    "params_sharding_tree = jax.tree.map(lambda _: replicated_sharding, abstract_params)\n",
    "print(\"\\nParameter Sharding Tree (all replicated):\")\n",
    "print(params_sharding_tree)\n",
    "\n",
    "# Initialize parameters using JIT with out_shardings to place them directly\n",
    "# onto devices with the specified (replicated) sharding.\n",
    "@functools.partial(jax.jit, out_shardings=params_sharding_tree)\n",
    "def initialize_params(key):\n",
    "    return model.init(key, dummy_device_input)['params']\n",
    "\n",
    "print(\"\\nInitializing parameters with replicated sharding...\")\n",
    "params = initialize_params(params_key)\n",
    "print(\"Parameter initialization complete.\")\n",
    "\n",
    "# Verify parameter sharding (optional check)\n",
    "print(\"Verifying Parameter Sharding:\")\n",
    "jax.tree_util.tree_map_with_path(\n",
    "    lambda path, x: print(f\"  Param: {path}, Shape: {x.shape}, Sharding: {x.sharding}\"),\n",
    "    params\n",
    ")\n",
    "# Check that all shardings are indeed the `replicated_sharding` object\n",
    "assert all(leaf.sharding == replicated_sharding\n",
    "           for leaf in jax.tree_util.tree_leaves(params))\n",
    "print(\"Parameter sharding verified.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fce9bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Getting abstract optimizer state structure...\n",
      "Abstract Opt State Structure:\n",
      "(ScaleByAdamState(count=ShapeDtypeStruct(shape=(), dtype=int32), mu={'dense_layer_1': {'bias': ShapeDtypeStruct(shape=(128,), dtype=float32), 'kernel': ShapeDtypeStruct(shape=(64, 128), dtype=float32)}, 'dense_layer_2': {'bias': ShapeDtypeStruct(shape=(10,), dtype=float32), 'kernel': ShapeDtypeStruct(shape=(128, 10), dtype=float32)}}, nu={'dense_layer_1': {'bias': ShapeDtypeStruct(shape=(128,), dtype=float32), 'kernel': ShapeDtypeStruct(shape=(64, 128), dtype=float32)}, 'dense_layer_2': {'bias': ShapeDtypeStruct(shape=(10,), dtype=float32), 'kernel': ShapeDtypeStruct(shape=(128, 10), dtype=float32)}}), EmptyState())\n",
      "\n",
      "Creating Optimizer State Sharding Tree (handling scalars)...\n",
      "Optimizer State Sharding Tree:\n",
      "(ScaleByAdamState(count=PartitionSpec(), mu={'dense_layer_1': {'bias': PartitionSpec(), 'kernel': PartitionSpec()}, 'dense_layer_2': {'bias': PartitionSpec(), 'kernel': PartitionSpec()}}, nu={'dense_layer_1': {'bias': PartitionSpec(), 'kernel': PartitionSpec()}, 'dense_layer_2': {'bias': PartitionSpec(), 'kernel': PartitionSpec()}}), EmptyState())\n",
      "\n",
      "Initializing optimizer state with corrected sharding...\n",
      "Optimizer state initialization complete.\n",
      "\n",
      "Verifying Optimizer State Sharding:\n",
      "  Opt State: (SequenceKey(idx=0), GetAttrKey(name='count')), Type: ArrayImpl, Shape: (), Sharding: NamedSharding(mesh=Mesh('data': 1, axis_types=(Auto,)), spec=PartitionSpec(), memory_kind=unpinned_host)\n",
      "  Opt State: (SequenceKey(idx=0), GetAttrKey(name='mu'), DictKey(key='dense_layer_1'), DictKey(key='bias')), Type: ArrayImpl, Shape: (128,), Sharding: NamedSharding(mesh=Mesh('data': 1, axis_types=(Auto,)), spec=PartitionSpec(), memory_kind=unpinned_host)\n",
      "  Opt State: (SequenceKey(idx=0), GetAttrKey(name='mu'), DictKey(key='dense_layer_1'), DictKey(key='kernel')), Type: ArrayImpl, Shape: (64, 128), Sharding: NamedSharding(mesh=Mesh('data': 1, axis_types=(Auto,)), spec=PartitionSpec(), memory_kind=unpinned_host)\n",
      "  Opt State: (SequenceKey(idx=0), GetAttrKey(name='mu'), DictKey(key='dense_layer_2'), DictKey(key='bias')), Type: ArrayImpl, Shape: (10,), Sharding: NamedSharding(mesh=Mesh('data': 1, axis_types=(Auto,)), spec=PartitionSpec(), memory_kind=unpinned_host)\n",
      "  Opt State: (SequenceKey(idx=0), GetAttrKey(name='mu'), DictKey(key='dense_layer_2'), DictKey(key='kernel')), Type: ArrayImpl, Shape: (128, 10), Sharding: NamedSharding(mesh=Mesh('data': 1, axis_types=(Auto,)), spec=PartitionSpec(), memory_kind=unpinned_host)\n",
      "  Opt State: (SequenceKey(idx=0), GetAttrKey(name='nu'), DictKey(key='dense_layer_1'), DictKey(key='bias')), Type: ArrayImpl, Shape: (128,), Sharding: NamedSharding(mesh=Mesh('data': 1, axis_types=(Auto,)), spec=PartitionSpec(), memory_kind=unpinned_host)\n",
      "  Opt State: (SequenceKey(idx=0), GetAttrKey(name='nu'), DictKey(key='dense_layer_1'), DictKey(key='kernel')), Type: ArrayImpl, Shape: (64, 128), Sharding: NamedSharding(mesh=Mesh('data': 1, axis_types=(Auto,)), spec=PartitionSpec(), memory_kind=unpinned_host)\n",
      "  Opt State: (SequenceKey(idx=0), GetAttrKey(name='nu'), DictKey(key='dense_layer_2'), DictKey(key='bias')), Type: ArrayImpl, Shape: (10,), Sharding: NamedSharding(mesh=Mesh('data': 1, axis_types=(Auto,)), spec=PartitionSpec(), memory_kind=unpinned_host)\n",
      "  Opt State: (SequenceKey(idx=0), GetAttrKey(name='nu'), DictKey(key='dense_layer_2'), DictKey(key='kernel')), Type: ArrayImpl, Shape: (128, 10), Sharding: NamedSharding(mesh=Mesh('data': 1, axis_types=(Auto,)), spec=PartitionSpec(), memory_kind=unpinned_host)\n",
      "Optimizer state sharding verified.\n"
     ]
    }
   ],
   "source": [
    "# Optimizer (Optax)\n",
    "optimizer = optax.adam(LEARNING_RATE)\n",
    "\n",
    "print(\"\\nGetting abstract optimizer state structure...\")\n",
    "abstract_opt_state = jax.eval_shape(optimizer.init, params)\n",
    "print(\"Abstract Opt State Structure:\")\n",
    "print(abstract_opt_state)\n",
    "\n",
    "def get_opt_state_sharding(leaf):\n",
    "    return NamedSharding(mesh, P())\n",
    "\n",
    "print(\"\\nCreating Optimizer State Sharding Tree (handling scalars)...\")\n",
    "opt_state_sharding_tree = jax.tree_util.tree_map(\n",
    "    get_opt_state_sharding, abstract_opt_state\n",
    ")\n",
    "print(\"Optimizer State Sharding Tree:\")\n",
    "print(jax.tree_util.tree_map(lambda x: x.spec, opt_state_sharding_tree)) \n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, out_shardings=opt_state_sharding_tree)\n",
    "def initialize_optimizer_state(p):\n",
    "    return optimizer.init(p)\n",
    "\n",
    "print(\"\\nInitializing optimizer state with corrected sharding...\")\n",
    "opt_state = initialize_optimizer_state(params)\n",
    "print(\"Optimizer state initialization complete.\")\n",
    "\n",
    "print(\"\\nVerifying Optimizer State Sharding:\")\n",
    "jax.tree_util.tree_map_with_path(\n",
    "     lambda path, x: print(f\"  Opt State: {path}, Type: {type(x).__name__}, \"\n",
    "                           f\"Shape: {getattr(x, 'shape', 'N/A')}, \"\n",
    "                           f\"Sharding: {getattr(x, 'sharding', 'N/A')}\"),\n",
    "     opt_state\n",
    ")\n",
    "\n",
    "def check_opt_state_leaf(actual_leaf, named_sharding_leaf):\n",
    "    intended_spec = named_sharding_leaf.spec \n",
    "\n",
    "    if hasattr(actual_leaf, 'sharding'):\n",
    "        assert actual_leaf.sharding.spec == intended_spec, \\\n",
    "            f\"Sharding mismatch for array: Actual={actual_leaf.sharding.spec}, Expected={intended_spec}\"\n",
    "    elif isinstance(actual_leaf, (int, jnp.integer, float, jnp.floating)):\n",
    "        assert intended_spec == P(), \\\n",
    "            f\"Sharding mismatch for scalar: Expected=P(), Got={intended_spec}\"\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "jax.tree_util.tree_map(check_opt_state_leaf, opt_state, opt_state_sharding_tree)\n",
    "\n",
    "print(\"Optimizer state sharding verified.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6fb58c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sharding initial data batch (Global shape: X=(32, 64), Y=(32,))\n",
      "Verifying data sharding:\n",
      "  Sharded X shape per device: (32, 64)\n",
      "  Sharded X sharding: NamedSharding(mesh=Mesh('data': 1, axis_types=(Auto,)), spec=PartitionSpec('data', None), memory_kind=unpinned_host)\n",
      "  Sharded Y shape per device: (32,)\n",
      "  Sharded Y sharding: NamedSharding(mesh=Mesh('data': 1, axis_types=(Auto,)), spec=PartitionSpec('data',), memory_kind=unpinned_host)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 5. Prepare and Shard Data Batch ---\n",
    "# Create *global* batch data on Host (CPU) first\n",
    "dummy_global_x = jax.random.normal(data_key, (GLOBAL_BATCH_SIZE, INPUT_FEATURES))\n",
    "# Use integers for typical classification labels\n",
    "dummy_global_y = jax.random.randint(data_key, (GLOBAL_BATCH_SIZE,), 0, OUTPUT_DIM)\n",
    "\n",
    "# Shard the global batch onto devices using jax.device_put\n",
    "# This transfers data from host and distributes it according to the specs.\n",
    "print(f\"\\nSharding initial data batch (Global shape: X={dummy_global_x.shape}, Y={dummy_global_y.shape})\")\n",
    "\n",
    "# It's often good practice to JIT the sharding if done repeatedly in a loop\n",
    "#@functools.partial(jax.jit, static_argnums=(1,2)) # Shardings are static\n",
    "def shard_batch(batch_data, data_named_sharding, label_named_sharding):\n",
    "    x, y = batch_data\n",
    "    x = jax.device_put(x, data_named_sharding)\n",
    "    y = jax.device_put(y, label_named_sharding)\n",
    "    return x, y\n",
    "\n",
    "sharded_x, sharded_y = shard_batch(\n",
    "    (dummy_global_x, dummy_global_y),\n",
    "    data_sharding,      # Use the data sharding spec P('data', None)\n",
    "    label_sharding      # Use the label sharding spec P('data',)\n",
    ")\n",
    "\n",
    "print(\"Verifying data sharding:\")\n",
    "print(f\"  Sharded X shape per device: {sharded_x.shape}\") # Shape reflects per-device slice\n",
    "print(f\"  Sharded X sharding: {sharded_x.sharding}\")\n",
    "assert sharded_x.sharding == data_sharding\n",
    "assert sharded_x.shape == (DEVICE_BATCH_SIZE, INPUT_FEATURES)\n",
    "\n",
    "print(f\"  Sharded Y shape per device: {sharded_y.shape}\")\n",
    "print(f\"  Sharded Y sharding: {sharded_y.sharding}\")\n",
    "assert sharded_y.sharding == label_sharding\n",
    "assert sharded_y.shape == (DEVICE_BATCH_SIZE,)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "370a4bc4",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (2588790154.py, line 69)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 69\u001b[39m\n\u001b[31m    \u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m incomplete input\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Define the Training Step ---\n",
    "\n",
    "# Loss function (cross-entropy) - operates on local batch\n",
    "def loss_fn(params, batch_x, batch_y, model_obj):\n",
    "    logits = model_obj.apply({'params': params}, batch_x)\n",
    "    one_hot_labels = jax.nn.one_hot(batch_y, num_classes=OUTPUT_DIM)\n",
    "    # Calculate loss on the local device's batch slice\n",
    "    loss = jnp.mean(optax.softmax_cross_entropy(logits=logits, labels=one_hot_labels))\n",
    "    return loss\n",
    "\n",
    "# Define the update step, JITted with sharding specifications\n",
    "@functools.partial(jax.jit,\n",
    "                   # Specify input shardings (must match how data/state IS sharded)\n",
    "                   in_shardings=(params_sharding_tree,  # Replicated\n",
    "                                 opt_state_sharding_tree,# Replicated\n",
    "                                 data_sharding,         # Sharded P('data', None)\n",
    "                                 label_sharding),       # Sharded P('data',)\n",
    "                   # Specify output shardings (how results SHOULD BE sharded)\n",
    "                   out_shardings=(params_sharding_tree, # Replicated\n",
    "                                  opt_state_sharding_tree,# Replicated\n",
    "                                  replicated_sharding)  # Loss (scalar, replicated)\n",
    "                   )\n",
    "def train_step(current_params, current_opt_state, batch_x, batch_y):\n",
    "    # Calculate loss and gradients on the local batch using the local param replica.\n",
    "    # Gradients will initially be replicated (like params).\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(current_params, batch_x, batch_y, model)\n",
    "\n",
    "    # *** CRUCIAL FOR REPLICATION / DATA PARALLELISM ***\n",
    "    # Average gradients across the 'data' mesh axis using pmean (parameter mean).\n",
    "    # This computes the mean of 'grads' across all devices in the 'data' axis.\n",
    "    # The result is replicated back to all devices in that axis.\n",
    "    averaged_grads = jax.lax.pmean(grads, axis_name='data')\n",
    "\n",
    "    # Also average the loss across devices for consistent reporting\n",
    "    averaged_loss = jax.lax.pmean(loss, axis_name='data')\n",
    "\n",
    "    # Compute updates using the optimizer (using the *averaged* gradients)\n",
    "    updates, new_opt_state = optimizer.update(averaged_grads, current_opt_state, current_params)\n",
    "\n",
    "    # Apply updates to parameters. Since params/updates are replicated,\n",
    "    # this happens identically on all devices, keeping them synchronized.\n",
    "    new_params = optax.apply_updates(current_params, updates)\n",
    "\n",
    "    return new_params, new_opt_state, averaged_loss\n",
    "\n",
    "# --- 7. Training Loop ---\n",
    "print(\"\\nStarting training loop...\")\n",
    "for step in range(NUM_STEPS):\n",
    "    # In a real loop, load and shard a *new* batch here.\n",
    "    # We reuse the same sharded batch for simplicity.\n",
    "    current_x = sharded_x\n",
    "    current_y = sharded_y\n",
    "\n",
    "    # Execute the JITted training step\n",
    "    params, opt_state, loss = train_step(params, opt_state, current_x, current_y)\n",
    "\n",
    "    if step % 10 == 0 or step == NUM_STEPS - 1:\n",
    "        # Loss is replicated, so we can just print it from any device (device 0 implicitly)\n",
    "        # Use .item() to get scalar value from the JAX array.\n",
    "        print(f\"Step: {step:3d}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# --- 8. Final Verification (Optional) ---\n",
    "print(\"\\nFinal Parameter Sharding Check:\")\n",
    "assert all(leaf.sharding == replicated_sharding\n",
    "           for leaf in jax.tree_util.tree_leaves(params))\n",
    "print(\"All parameters remain replicated.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9737eab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
