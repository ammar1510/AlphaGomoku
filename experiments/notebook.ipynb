{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2682d745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import lax\n",
    "from typing import Tuple, Dict, Any, NamedTuple, Optional\n",
    "from functools import partial\n",
    "from jax.sharding import Mesh, NamedSharding, PartitionSpec as PS\n",
    "from dataclasses import dataclass\n",
    "from jax import jit,lax,tree\n",
    "import distrax\n",
    "\n",
    "os.environ['XLA_FLAGS'] = '--xla_force_host_platform_device_count=8'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "163277d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]\n"
     ]
    }
   ],
   "source": [
    "print(jax.devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11b283ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = Mesh(jax.devices(),axis_names=(\"batch\"))\n",
    "\n",
    "def mesh_sharding(*names: str | None) -> NamedSharding:\n",
    "    return NamedSharding(mesh, PS(*names))\n",
    "\n",
    "@dataclass(unsafe_hash=True)\n",
    "class MeshRules:\n",
    "    batch: tuple[str | None, ...]\n",
    "    replicated: tuple[str | None, ...]\n",
    "    buffer: tuple[str | None, ...]\n",
    "    \n",
    "    def __call__(self, name: str) -> NamedSharding:\n",
    "        sharding_spec = getattr(self, name)\n",
    "        return mesh_sharding(*sharding_spec)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecbf7709",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_rules = MeshRules(\n",
    "    batch=('batch',),\n",
    "    replicated=(),  # Empty tuple for no sharding\n",
    "    buffer=(None, 'batch')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cde06abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENV RELATED COMPONENTS \n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from typing import Tuple, Dict, Any, NamedTuple\n",
    "\n",
    "# This is the state definition from the original environment.\n",
    "# It's needed for type hints and for the methods to return the correct state structure.\n",
    "class EnvState(NamedTuple):\n",
    "    \"\"\"Holds the dynamic state of the batched Gomoku environment for a single step.\"\"\"\n",
    "    boards: jnp.ndarray  # (B, board_size, board_size) float32 tensor\n",
    "    current_players: jnp.ndarray  # (B,) int32 tensor (1 or -1)\n",
    "    dones: jnp.ndarray  # (B,) bool tensor\n",
    "    winners: jnp.ndarray  # (B,) int32 tensor (1, -1, or 0 for draw/ongoing)\n",
    "\n",
    "\n",
    "WIN_LENGTH = 5 # Default from original environment\n",
    "\n",
    "class DummyEnv:\n",
    "    \"\"\"\n",
    "    A dummy version of Env with minimal logic, focusing on compatible shapes.\n",
    "    Observations are player agnostic.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, B: int, board_size: int = 9, win_length: int = WIN_LENGTH):\n",
    "        \"\"\"\n",
    "        Initializes the dummy environment configuration.\n",
    "\n",
    "        Args:\n",
    "            B: Batch size.\n",
    "            board_size: The size of the Gomoku board.\n",
    "            win_length: The number of consecutive pieces needed to win (unused in dummy logic).\n",
    "        \"\"\"\n",
    "        # self.B is set by JaxEnvBase\n",
    "        self.B = B\n",
    "        self.board_size = board_size\n",
    "        self.win_length = win_length # Stored for compatibility, but not used by dummy logic\n",
    "\n",
    "    def init_state(self) -> EnvState:\n",
    "        \"\"\"\n",
    "        Creates an initial dummy EnvState.\n",
    "        \"\"\"\n",
    "        return lax.with_sharding_constraint(EnvState(\n",
    "            boards=jnp.zeros((self.B, self.board_size, self.board_size), dtype=jnp.float32),\n",
    "            current_players=jnp.ones((self.B,), dtype=jnp.int32), # Player 1 starts\n",
    "            dones=jnp.zeros((self.B,), dtype=jnp.bool_),\n",
    "            winners=jnp.zeros((self.B,), dtype=jnp.int32),\n",
    "        ),mesh_rules(\"batch\"))\n",
    "\n",
    "    def reset(\n",
    "        self\n",
    "    ) -> Tuple[EnvState, jnp.ndarray, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Resets environments to initial dummy states.\n",
    "        \"\"\"\n",
    "        new_state = self.init_state()\n",
    "        initial_observations = new_state.boards # Shape: (B, board_size, board_size)\n",
    "        info = {}\n",
    "        return new_state, initial_observations, info\n",
    "\n",
    "    def step(\n",
    "        self, state: EnvState, actions: jnp.ndarray\n",
    "    ) -> Tuple[EnvState, jnp.ndarray, jnp.ndarray, jnp.ndarray, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Takes a dummy step. Switches player, returns zero rewards and current dones.\n",
    "        Board state does not change. Actions are ignored.\n",
    "\n",
    "        Args:\n",
    "            state: The current GomokuState.\n",
    "            actions: JAX array of actions (row, col). Shape (B, 2). (Ignored by dummy)\n",
    "\n",
    "        Returns:\n",
    "            A tuple (new_state, observations, rewards, dones, info).\n",
    "        \"\"\"\n",
    "        # Dummy state update: just flip player, keep board, rng, and dones the same\n",
    "        # If you need dones to eventually become true for testing, you'd add logic here.\n",
    "        # For a simple dummy, keeping dones as they are (initially all False) is okay.\n",
    "        # Or, for testing termination, you could do:\n",
    "        # rng_step, _ = jax.random.split(state.rng)\n",
    "        # new_dones = jax.random.bernoulli(rng_step, 0.1, (self.B,)) # Example: 10% chance of ending\n",
    "        new_dones = state.dones # Keep dones as they are for simplicity\n",
    "\n",
    "        new_state = state._replace(dones=new_dones)          \n",
    "        observations = new_state.boards # Shape: (B, board_size, board_size)\n",
    "        rewards = jnp.zeros((self.B,), dtype=jnp.float32) # No rewards\n",
    "        \n",
    "        info = {}\n",
    "        return new_state, observations, rewards, new_dones, info\n",
    "\n",
    "    def initialize_trajectory_buffers(self, max_steps: int) -> Tuple[jnp.ndarray, ...]:\n",
    "        \"\"\"\n",
    "        Creates and returns pre-allocated JAX arrays for storing trajectory data.\n",
    "        This is mostly shape-dependent and matches the original.\n",
    "        \"\"\"\n",
    "        obs_shape = self.observation_shape\n",
    "        act_shape = self.action_shape\n",
    "\n",
    "        observations = jnp.zeros((max_steps, self.B) + obs_shape, dtype=jnp.float32)\n",
    "        actions = jnp.zeros((max_steps, self.B) + act_shape, dtype=jnp.int32)\n",
    "        # values buffer is often T+1 for GAE\n",
    "        values = jnp.zeros((max_steps + 1, self.B), dtype=jnp.float32) \n",
    "        rewards = jnp.zeros((max_steps, self.B), dtype=jnp.float32)\n",
    "        dones = jnp.zeros((max_steps, self.B), dtype=jnp.bool_)\n",
    "        log_probs = jnp.zeros((max_steps, self.B), dtype=jnp.float32)\n",
    "        current_players_buffer = jnp.zeros((max_steps, self.B), dtype=jnp.int32)\n",
    "\n",
    "        sharded_output = tree.map(lambda x: lax.with_sharding_constraint(x,mesh_rules(\"buffer\")), (observations, actions, values, rewards, dones, log_probs, current_players_buffer))\n",
    "        return sharded_output\n",
    "\n",
    "    @property\n",
    "    def observation_shape(self) -> tuple:\n",
    "        \"\"\"Returns the shape of a single observation (board state).\"\"\"\n",
    "        return (self.board_size, self.board_size)\n",
    "\n",
    "    @property\n",
    "    def action_shape(self) -> tuple:\n",
    "        \"\"\"Returns the shape of a single action (row, col).\"\"\"\n",
    "        return (2,) # (row, col)\n",
    "\n",
    "    def get_action_mask(self, state: EnvState) -> jnp.ndarray:\n",
    "        \"\"\"\n",
    "        Returns a dummy boolean mask of valid actions.\n",
    "        All actions are considered valid if the game is not done.\n",
    "        \"\"\"\n",
    "        # All positions are valid if not done.\n",
    "        # Shape: (B, board_size, board_size)\n",
    "        mask = jnp.ones((self.B, self.board_size, self.board_size), dtype=jnp.bool_)\n",
    "        # Respect existing done flags\n",
    "        return mask & (~state.dones[:, None, None])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92b342eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoopState(NamedTuple):\n",
    "    state: EnvState\n",
    "    obs: jnp.ndarray\n",
    "    observations: jnp.ndarray\n",
    "    values: jnp.ndarray\n",
    "    actions: jnp.ndarray\n",
    "    rewards: jnp.ndarray\n",
    "    dones: jnp.ndarray\n",
    "    logprobs: jnp.ndarray\n",
    "    current_players: jnp.ndarray  \n",
    "    step_idx: int\n",
    "    rng: jax.random.PRNGKey\n",
    "    termination_step_indices: (\n",
    "        jnp.ndarray\n",
    "    )  # Stores the step index 't' when done first becomes True for each batch element\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "149a5f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@partial(jit, static_argnames=[\"env\", \"actor_critic\"])\n",
    "def player_move(\n",
    "    loop_state: LoopState, env, actor_critic, params\n",
    ") -> LoopState:\n",
    "    \"\"\"Takes a single step in the environment using the provided actor-critic.\"\"\"\n",
    "    current_state: EnvState = loop_state.state\n",
    "    current_obs: jnp.ndarray = loop_state.obs\n",
    "    current_player: jnp.ndarray = (\n",
    "        current_state.current_players\n",
    "    ) \n",
    "    step_idx: int = loop_state.step_idx\n",
    "    rng = loop_state.rng\n",
    "\n",
    "    # Get policy distribution and value from the model\n",
    "    pi_dist, value = actor_critic.apply(\n",
    "        {\"params\": params}, current_obs, current_player\n",
    "    ) \n",
    "    lax.with_sharding_constraint(value,mesh_rules(\"batch\"))\n",
    "    lax.with_sharding_constraint(pi_dist.logits,mesh_rules(\"batch\"))\n",
    "\n",
    "    # Get action mask from the environment\n",
    "    action_mask = env.get_action_mask(current_state)  # (B, H, W)\n",
    "    jax.debug.visualize_array_sharding(action_mask[:,:,0])\n",
    "    B, H, W = action_mask.shape\n",
    "    flat_action_mask = action_mask.reshape(B, -1)  # (B, H*W)\n",
    "\n",
    "    # Get original logits from the distribution\n",
    "    original_logits = pi_dist.logits  # Shape (B, H*W)\n",
    "\n",
    "    # Apply the mask to the logits\n",
    "    masked_logits = jnp.where(flat_action_mask, original_logits, -jnp.inf)\n",
    "\n",
    "    # Create a new distribution with masked logits\n",
    "    masked_pi_dist = distrax.Categorical(logits=masked_logits)\n",
    "\n",
    "    # Sample action from the masked distribution\n",
    "    rng, subkey = jax.random.split(rng)\n",
    "    flat_action = masked_pi_dist.sample(seed=subkey)  # Shape (B,)\n",
    "    logprob = masked_pi_dist.log_prob(flat_action)  # Shape (B,)\n",
    "\n",
    "    # Convert flat action back to (row, col) for the environment step\n",
    "    action_row = flat_action // W\n",
    "    action_col = flat_action % W\n",
    "    action = jnp.stack([action_row, action_col], axis=-1)  # Shape (B, 2)\n",
    "\n",
    "\n",
    "    observations = loop_state.observations.at[step_idx].set(current_obs)\n",
    "    actions = loop_state.actions.at[step_idx].set(action)\n",
    "    values = loop_state.values.at[step_idx].set(value)\n",
    "    logprobs = loop_state.logprobs.at[step_idx].set(logprob)\n",
    "    current_players = loop_state.current_players.at[step_idx].set(current_player)\n",
    "\n",
    "    next_state, next_obs, step_rewards, dones, info = env.step(current_state, action)\n",
    "\n",
    "    rewards = loop_state.rewards.at[step_idx].set(step_rewards)\n",
    "    dones_buffer = loop_state.dones.at[step_idx].set(dones)\n",
    "\n",
    "    # Update termination indices: if not already terminated and current step is done, record step_idx\n",
    "    current_termination_indices = loop_state.termination_step_indices\n",
    "    not_terminated_yet = current_termination_indices == jnp.iinfo(jnp.int32).max\n",
    "    new_termination_indices = jnp.where(\n",
    "        not_terminated_yet & dones,\n",
    "        step_idx,  # Record current step index as termination step\n",
    "        current_termination_indices,  # Keep existing index (either max or previously recorded step)\n",
    "    )\n",
    "\n",
    "    return loop_state._replace(\n",
    "        state=next_state,\n",
    "        obs=next_obs,\n",
    "        observations=observations,\n",
    "        actions=actions,\n",
    "        values=values,\n",
    "        rewards=rewards,\n",
    "        dones=dones_buffer,\n",
    "        logprobs=logprobs,\n",
    "        current_players=current_players,\n",
    "        step_idx=step_idx + 1,\n",
    "        rng=rng,\n",
    "        termination_step_indices=new_termination_indices,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30593458",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@partial(\n",
    "    jit,\n",
    "    static_argnames=[\"env\", \"black_actor_critic\", \"white_actor_critic\", \"buffer_size\"],\n",
    ")\n",
    "def run_episode(\n",
    "    env,\n",
    "    black_actor_critic,\n",
    "    black_params,\n",
    "    white_actor_critic,\n",
    "    white_params,\n",
    "    rng,\n",
    "    buffer_size\n",
    ") -> Tuple[\n",
    "    Dict[str, Any], EnvState, jax.random.PRNGKey\n",
    "]:  # Return full buffers, final_state, rng\n",
    "    \"\"\"\n",
    "    Collect trajectories for self-play with separate black and white models.\n",
    "    Runs until all environments are done.\n",
    "    Buffers are allocated based on buffer_size.\n",
    "\n",
    "    Args:\n",
    "        env: An instance of JaxEnvBase. Assumes player 1 is \"black\".\n",
    "        black_actor_critic: ActorCritic model for black player (first player).\n",
    "        black_params: Parameters for black player model.\n",
    "        white_actor_critic: ActorCritic model for white player (second player).\n",
    "        white_params: Parameters for white player model.\n",
    "        rng: JAX RNG key.\n",
    "        buffer_size: The size of the trajectory buffers to allocate.\n",
    "\n",
    "    Returns:\n",
    "        full_trajectory: Dict containing the full, un-sliced buffers (observations, actions, rewards, dones, logprobs, current_players, valid_mask, T, termination_indices).\n",
    "                         Arrays have length buffer_size.\n",
    "        final_state: The environment state after the final step taken in the loop.\n",
    "        rng: updated RNG key.\n",
    "\n",
    "    Note: This function can simulate the behavior of `run_selfplay` function\n",
    "          by passing the same actor_critic model and params for both the black and white players.\n",
    "    \"\"\"\n",
    "    initial_state, initial_obs, _ = env.reset()\n",
    "\n",
    "    buffers = env.initialize_trajectory_buffers(buffer_size)\n",
    "    observations, actions, values, rewards, dones_buffer, logprobs, current_players_buffer = (\n",
    "        buffers  # Unpack players buffer\n",
    "    )\n",
    "    B = initial_obs.shape[0]  # Infer batch size\n",
    "    initial_termination_indices = lax.with_sharding_constraint(jnp.full(\n",
    "        (B,), jnp.iinfo(jnp.int32).max, dtype=jnp.int32\n",
    "    ),mesh_rules(\"batch\"))\n",
    "\n",
    "    initial_loop_state = LoopState(\n",
    "        state=initial_state,\n",
    "        obs=initial_obs,\n",
    "        observations=observations,\n",
    "        actions=actions,\n",
    "        values=values,\n",
    "        rewards=rewards,\n",
    "        dones=dones_buffer,\n",
    "        logprobs=logprobs,\n",
    "        current_players=current_players_buffer, \n",
    "        step_idx=0,\n",
    "        rng=rng,\n",
    "        termination_step_indices=initial_termination_indices,  # Initialize termination indices\n",
    "    )\n",
    "\n",
    "    def cond_fn(l_state: LoopState) -> bool:\n",
    "        return ~jnp.all(l_state.state.dones)\n",
    "\n",
    "    @partial(jit, static_argnames=[\"env\", \"black_actor_critic\", \"white_actor_critic\"])\n",
    "    def body_fn_alternating(\n",
    "        l_state: LoopState,\n",
    "        env,\n",
    "        black_actor_critic,\n",
    "        black_params,\n",
    "        white_actor_critic,\n",
    "        white_params,\n",
    "    ) -> LoopState:\n",
    "        current_step = l_state.step_idx\n",
    "        is_black_turn = current_step % 2 == 0\n",
    "\n",
    "        return jax.lax.cond(\n",
    "            is_black_turn,\n",
    "            lambda s: player_move(s, env, black_actor_critic, black_params),\n",
    "            lambda s: player_move(s, env, white_actor_critic, white_params),\n",
    "            l_state,\n",
    "        )\n",
    "\n",
    "    def body_fn_wrapped(l_state: LoopState) -> LoopState:\n",
    "        # Pass static args explicitly if needed by jit context, or rely on closure\n",
    "        return body_fn_alternating(\n",
    "            l_state,\n",
    "            env,\n",
    "            black_actor_critic,\n",
    "            black_params,\n",
    "            white_actor_critic,\n",
    "            white_params,\n",
    "        )\n",
    "\n",
    "    final_state = lax.while_loop(cond_fn, body_fn_wrapped, initial_loop_state)\n",
    "\n",
    "    #final value needed for GAE calculation\n",
    "    final_value = final_state.values[-1]\n",
    "    final_values = final_state.values.at[-1].set(final_value)\n",
    "\n",
    "\n",
    "\n",
    "    term_indices = final_state.termination_step_indices  # Shape (B,)\n",
    "    T = final_state.step_idx  # Use actual steps taken up to buffer_size\n",
    "    B = initial_obs.shape[0]\n",
    "\n",
    "    # Ensure T is used correctly for the mask dimensions even if less than buffer_size\n",
    "    step_indices = jnp.arange(buffer_size)[:, None]  # Shape (buffer_size, 1)\n",
    "\n",
    "    # Broadcast comparison: mask is True if step_index <= termination_index\n",
    "    # Using '<=' ensures the terminal step itself is included as valid\n",
    "    # We create a mask for the full buffer size\n",
    "    valid_mask = step_indices <= term_indices[None, :]  # Shape (buffer_size, B)\n",
    "\n",
    "    full_trajectory = {\n",
    "        # Use the full buffers\n",
    "        \"observations\": final_state.observations,  # Shape (buffer_size, B, ...)\n",
    "        \"actions\": final_state.actions,  # Shape (buffer_size, B, ...)\n",
    "        \"values\": final_values,  # Shape (buffer_size+1, B)\n",
    "        \"rewards\": final_state.rewards,  # Shape (buffer_size, B)\n",
    "        \"dones\": final_state.dones,  # Shape (buffer_size, B)\n",
    "        \"logprobs\": final_state.logprobs,  # Shape (buffer_size, B)\n",
    "        \"current_players\": final_state.current_players,  # Add stored players\n",
    "        \"valid_mask\": valid_mask,  # Add the calculated mask, shape (buffer_size, B)\n",
    "        \"T\": T,  # Actual number of steps executed (can be less than buffer_size)\n",
    "        \"termination_step_indices\": final_state.termination_step_indices,  # Keep this too if needed elsewhere\n",
    "    }\n",
    "    rng = final_state.rng\n",
    "\n",
    "    # Return the final EnvState directly, contains final_obs and final_player\n",
    "    return full_trajectory, final_state.state, rng\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68c6ab82",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def calculate_gae(\n",
    "    rewards: jnp.ndarray,\n",
    "    values: jnp.ndarray,\n",
    "    dones: jnp.ndarray,\n",
    "    gamma: float = 0.99,\n",
    "    gae_lambda: float = 0.95,\n",
    ") -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Compute Generalized Advantage Estimation (GAE) using lax.scan directly on batched data.\n",
    "\n",
    "    Args:\n",
    "        rewards: Rewards array, shape (T, B).\n",
    "        values: Value estimates, shape (T+1, B). Include value of *terminal* state.\n",
    "        dones: Done flags, shape (T, B). Dones resulting from the action at step t.\n",
    "        gamma: Discount factor.\n",
    "        gae_lambda: GAE lambda parameter.\n",
    "\n",
    "    Returns:\n",
    "        advantages: GAE advantages, shape (T, B).\n",
    "        returns: GAE-based returns (advantages + values), shape (T, B).\n",
    "    \"\"\"\n",
    "    T = rewards.shape[0]\n",
    "    B = rewards.shape[1]\n",
    "    assert (\n",
    "        values.shape[0] == T + 1\n",
    "    ), f\"Values should have shape ({T+1}, B), but got {values.shape}\"\n",
    "    assert (\n",
    "        values.shape[1] == B\n",
    "    ), f\"Values batch dimension mismatch: {values.shape[1]} vs {B}\"\n",
    "    assert (\n",
    "        dones.shape[0] == T\n",
    "    ), f\"Dones time dimension mismatch: {dones.shape[0]} vs {T}\"\n",
    "    assert (\n",
    "        dones.shape[1] == B\n",
    "    ), f\"Dones batch dimension mismatch: {dones.shape[1]} vs {B}\"\n",
    "\n",
    "    values_t = values[:-1]  # V(s_0)...V(s_{T-1}), shape (T, B)\n",
    "    values_tp1 = values[1:]  # V(s_1)...V(s_T), shape (T, B)\n",
    "    dones = dones.astype(jnp.float32)  # Ensure float, shape (T, B)\n",
    "\n",
    "    # Calculate deltas: delta_t = r_t - gamma * V(s_{t+1}) * (1 - d_t)\n",
    "    # minus sign as the next value is wrt to opponent\n",
    "    # not sure if this is correct\n",
    "    deltas = rewards - gamma * values_tp1 * (1.0 - dones) - values_t  # Shape (T, B)\n",
    "\n",
    "    def scan_fn(carry_gae_batch, step_data_batch):\n",
    "        # carry_gae_batch: shape (B,) - Represents A_{t+1} from the opponent's perspective\n",
    "        # step_data_batch: tuple (delta_batch, done_batch), each shape (B,)\n",
    "        delta_batch, done_batch = step_data_batch\n",
    "\n",
    "        # Calculate GAE for the batch: A_t = delta_t - gamma * lambda * A_{t+1} * (1 - d_t)\n",
    "        # Subtract the opponent's advantage A_{t+1} (carry_gae_batch) because of the zero-sum game\n",
    "        # All operations are element-wise across the batch dimension.\n",
    "        gae_batch = (\n",
    "            delta_batch - gamma * gae_lambda * (1.0 - done_batch) * carry_gae_batch\n",
    "        )  # Shape (B,)\n",
    "\n",
    "        # Return the new carry (current GAE) and the value to store (also current GAE)\n",
    "        return gae_batch, gae_batch\n",
    "\n",
    "    # Prepare inputs for scan over time axis (0)\n",
    "    # Scan operates on the leading dimension T.\n",
    "    scan_inputs = (deltas, dones)  # Structure: ((T, B), (T, B))\n",
    "\n",
    "    # Initial carry state for the scan needs to match the batch dimension\n",
    "    initial_carry = jnp.zeros(B)  # Shape (B,)\n",
    "\n",
    "    # Scan over axis 0 (time) in reverse.\n",
    "    # Inputs structure ((T, B), (T, B)), step_data_batch will be ((B,), (B,))\n",
    "    # Carry has shape (B,). Output ys will have shape (T, B).\n",
    "    # lax.scan with reverse=True processes inputs from T-1 down to 0,\n",
    "    # but returns the collected outputs in the original order (0..T-1).\n",
    "    _, advantages = lax.scan(scan_fn, initial_carry, scan_inputs, reverse=True)\n",
    "\n",
    "    # Calculate returns: R_t = A_t + V(s_t)\n",
    "    returns = advantages + values_t  # Shape (T, B)\n",
    "\n",
    "    return advantages, returns\n",
    "\n",
    "@staticmethod\n",
    "@partial(jax.jit,out_shardings=mesh_rules(\"buffer\"),static_argnames=(\"gamma\",\"gae_lambda\"))\n",
    "def compute_gae_targets(\n",
    "    rewards: jnp.ndarray,\n",
    "    values: jnp.ndarray,\n",
    "    dones: jnp.ndarray,\n",
    "    gamma: float,\n",
    "    gae_lambda: float,\n",
    ") -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
    "    \"\"\"\n",
    "    Calculates Generalized Advantage Estimation (GAE) and returns (targets for value function).\n",
    "    The returned advantages are normalized (mean=0, std=1) over the batch.\n",
    "\n",
    "    Args:\n",
    "        rewards: Sequence of rewards, shape (T, B) or (T,).\n",
    "        values: Sequence of value estimates V(s_t), including V(s_T), shape (T+1, B) or (T+1,).\n",
    "        dones: Sequence of done flags, shape (T, B) or (T,).\n",
    "        gamma: Discount factor.\n",
    "        gae_lambda: GAE lambda parameter.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (normalized_advantages, returns)\n",
    "                - normalized_advantages: Normalized GAE estimates, shape (T, B) or (T,).\n",
    "                - returns: Target values for the value function, shape (T, B) or (T,).\n",
    "    \"\"\"\n",
    "    advantages_raw, returns = calculate_gae(\n",
    "        rewards, values, dones, gamma, gae_lambda\n",
    "    )\n",
    "\n",
    "    # Normalize advantages over the batch\n",
    "    advantages_mean = advantages_raw.mean()\n",
    "    advantages_std = (\n",
    "        advantages_raw.std() + 1e-8\n",
    "    )  # Add epsilon for numerical stability\n",
    "    advantages_normalized = (advantages_raw - advantages_mean) / advantages_std\n",
    "\n",
    "    return advantages_normalized, returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ff1c9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from typing import Tuple\n",
    "import distrax\n",
    "\n",
    "class SimpleActorCritic(nn.Module):\n",
    "    board_size: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(\n",
    "        self, x: jnp.ndarray, current_players: jnp.ndarray\n",
    "    ) -> Tuple[distrax.Categorical, jnp.ndarray]:\n",
    "        \"\"\"\n",
    "        Simplified forward pass with dummy outputs maintaining shapes.\n",
    "        Includes a dummy parameter to ensure 'params' collection is created.\n",
    "        \"\"\"\n",
    "        # Add a dummy parameter to ensure 'params' collection is created by init\n",
    "        # This parameter is not used in the actual computation.\n",
    "        _ = self.param('dummy_param', nn.initializers.zeros, (1,))\n",
    "\n",
    "        prefix_shape = x.shape[:-2]  # e.g., (batch,) or (T, batch) or ()\n",
    "        \n",
    "        num_actions = self.board_size * self.board_size\n",
    "        # Using jnp.ones for logits to make all actions equally likely before masking\n",
    "        dummy_policy_logits = jnp.ones(prefix_shape + (num_actions,)) \n",
    "        pi = distrax.Categorical(logits=dummy_policy_logits)\n",
    "\n",
    "        dummy_value = jnp.zeros(prefix_shape) # Dummy value: (...)\n",
    "\n",
    "        return pi, dummy_value\n",
    "\n",
    "    def evaluate_actions(\n",
    "        self, obs: jnp.ndarray, current_players: jnp.ndarray, actions: jnp.ndarray\n",
    "    ) -> Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray]:\n",
    "        \"\"\"\n",
    "        Simplified evaluate_actions with dummy outputs.\n",
    "        \"\"\"\n",
    "        pi, value = self(obs, current_players) # This will use the __call__ with the dummy param\n",
    "\n",
    "        prefix_shape = obs.shape[:-2]\n",
    "        \n",
    "        # Dummy log_prob, entropy, and value with correct shapes\n",
    "        dummy_log_prob = jnp.zeros(prefix_shape) \n",
    "        dummy_entropy = jnp.zeros(prefix_shape)\n",
    "        \n",
    "        return dummy_log_prob, dummy_entropy, value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9214278e",
   "metadata": {},
   "outputs": [],
   "source": [
    "black_model = SimpleActorCritic(board_size=15)\n",
    "white_model = SimpleActorCritic(board_size=15)\n",
    "rng = jax.random.PRNGKey(0)\n",
    "model_rng_b, model_rng_w = jax.random.split(rng)\n",
    "dummy_obs = jnp.zeros((1,15,15))\n",
    "dummy_player = jnp.ones((1,), dtype=jnp.int32)\n",
    "\n",
    "black_params = black_model.init(model_rng_b, dummy_obs, dummy_player)[\"params\"]\n",
    "white_params = white_model.init(model_rng_w, dummy_obs, dummy_player)[\"params\"]\n",
    "\n",
    "black_params = jax.device_put(black_params,mesh_rules(\"replicated\"))\n",
    "white_params = jax.device_put(white_params,mesh_rules(\"replicated\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e19de793",
   "metadata": {},
   "outputs": [],
   "source": [
    "board_size = 15\n",
    "action_shape = (2,)\n",
    "buffer_size = board_size * board_size\n",
    "batch_size = 64\n",
    "\n",
    "env = DummyEnv(batch_size,board_size,5)\n",
    "rng = jax.random.PRNGKey(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b928e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">  TPU 0  </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">  TPU 1  </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">  TPU 2  </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">  TPU 3  </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">         </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">         </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;255;255;48;2;57;59;121m  \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mTPU 0\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m  \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m  \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214mTPU 1\u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m  \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m  \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74mTPU 2\u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m  \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m  \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107mTPU 3\u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m  \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m         \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m         \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m         \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">  TPU 0  </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">  TPU 1  </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">  TPU 2  </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">  TPU 3  </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">         </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">         </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;255;255;48;2;57;59;121m  \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mTPU 0\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m  \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m  \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214mTPU 1\u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m  \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m  \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74mTPU 2\u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m  \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m  \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107mTPU 3\u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m  \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m         \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m         \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m         \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "full_trajectory, final_env_state, _ = run_episode( # Rollout RNG is consumed here\n",
    "    env=env,\n",
    "    black_actor_critic=black_model,\n",
    "    black_params=black_params,\n",
    "    white_actor_critic=white_model,\n",
    "    white_params=white_params,\n",
    "    rng=rng,\n",
    "    buffer_size=buffer_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54db03ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(225, 64, 15, 15)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_trajectory[\"observations\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba047589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">         </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">         </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">         </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">         </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">         </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">  TPU 0  </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">  TPU 1  </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">  TPU 2  </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">  TPU 3  </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">         </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">         </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">         </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">         </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">         </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">         </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m         \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m         \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m         \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m         \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m         \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m         \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m         \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m         \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m         \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m         \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m  \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mTPU 0\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m  \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m  \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214mTPU 1\u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m  \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m  \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74mTPU 2\u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m  \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m  \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107mTPU 3\u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m  \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m         \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m         \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m         \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m         \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m         \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m         \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m         \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m         \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m         \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m         \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m         \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jax.debug.visualize_array_sharding(full_trajectory[\"rewards\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fbbb2bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">         </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">         </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">         </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">         </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">         </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">  TPU 0  </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">  TPU 1  </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">  TPU 2  </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">  TPU 3  </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">         </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">         </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">         </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">         </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">         </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6\">         </span><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a\">         </span><span style=\"color: #000000; text-decoration-color: #000000; background-color: #b5cf6b\">         </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m         \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m         \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m         \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m         \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m         \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m         \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m         \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m         \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m         \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m         \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m  \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mTPU 0\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m  \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m  \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214mTPU 1\u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m  \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m  \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74mTPU 2\u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m  \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m  \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107mTPU 3\u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m  \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m         \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m         \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m         \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m         \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m         \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m         \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m         \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m         \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m         \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m         \u001b[0m\u001b[38;2;255;255;255;48;2;222;158;214m         \u001b[0m\u001b[38;2;255;255;255;48;2;173;73;74m         \u001b[0m\u001b[38;2;0;0;0;48;2;181;207;107m         \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def f(x):\n",
    "    return lax.with_sharding_constraint(x,mesh_rules(\"buffer\"))\n",
    "\n",
    "\n",
    "rng = jax.random.PRNGKey(0)\n",
    "arr = jax.random.normal(rng, (1024,1024))\n",
    "arr = f(arr)\n",
    "jax.debug.visualize_array_sharding(arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243c5e98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
